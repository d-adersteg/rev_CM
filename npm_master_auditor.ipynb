{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# -----------------------------------------------------------------------------\n",
    "# PURPOSE: NIAP TO PRODUCT MAPPING\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "\n",
    "# 1. Configuration\n",
    "TARGET_FOLDER_ID = \"15xKraT4MhWzBvOjMTJ0x21TBIopqfl4E\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Artifacts\n",
    "CSV_JIRA = f\"raw_jira_initiatives_{TIMESTAMP}.csv\"\n",
    "CSV_REG  = f\"raw_niap_register_{TIMESTAMP}.csv\"\n",
    "CSV_CORE = f\"raw_core_inventory_{TIMESTAMP}.csv\"\n",
    "CSV_MASTER = f\"master_reconciliation_table_{TIMESTAMP}.csv\"\n",
    "JSON_METRICS = f\"data_overview_metrics_{TIMESTAMP}.json\"\n",
    "JSON_ANALYSIS = f\"mapping_metrics_analysis_{TIMESTAMP}.json\"\n",
    "REPORT_MD = f\"audit_analysis_report_{TIMESTAMP}.md\"\n",
    "TXT_METADATA = f\"audit_provenance_metadata_{TIMESTAMP}.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UTILITIES\n",
    "# -----------------------------------------------------------------------------\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        if isinstance(obj, np.floating): return float(obj)\n",
    "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def clean_technical_name(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text).upper()\n",
    "    text = re.sub(r'_(GB|EUR|US|AU|NZ|SG|JP|RO|FR|PL|ES|IT|IE)_?', ' ', text)\n",
    "    identifiers = [r'PT_', r'MF_', r'ACQ_', r'REVX_', r'_REPRICING', r'_PLAN', r'_BASE', r'_STD', r'_PREM', r'_METAL']\n",
    "    for pattern in identifiers: \n",
    "        text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub(r'[^A-Z0-9\\s]', ' ', text) \n",
    "    return \" \".join(text.split()).lower()\n",
    "\n",
    "class SmartMatcher:\n",
    "    def __init__(self, source_list):\n",
    "        self.original_source = [str(x) for x in source_list if pd.notna(x)]\n",
    "        self.clean_source = [utils.default_process(x) for x in self.original_source]\n",
    "\n",
    "    def match(self, query_list, threshold=82):\n",
    "        results = []\n",
    "        for query in query_list:\n",
    "            if not query or str(query).strip() == \"\":\n",
    "                results.append({\"best_match\": None, \"score\": 0})\n",
    "                continue\n",
    "            match = process.extractOne(utils.default_process(str(query)), self.clean_source, scorer=fuzz.WRatio, processor=None)\n",
    "            if match and match[1] >= threshold:\n",
    "                results.append({\"best_match\": self.original_source[match[2]], \"score\": round(match[1], 2)})\n",
    "            else:\n",
    "                results.append({\"best_match\": None, \"score\": 0})\n",
    "        return results\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# REPORT GENERATORS\n",
    "# -----------------------------------------------------------------------------\n",
    "def generate_audit_manifest(exec_time, sql_map):\n",
    "    content = f\"\"\"NIAP AUDIT: FORENSIC METADATA MANIFEST\n",
    "================================================================================\n",
    "EXECUTION TIMESTAMP: {exec_time}\n",
    "ALGORITHM: RapidFuzz (WRatio) | THRESHOLD: 82%\n",
    "NORMALIZATION REGEX: r'_(GB|EUR|US...)', r'PT_|MF_|ACQ_|REVX_', r'_REPRICING|_PLAN...'\n",
    "\n",
    "DATA PROVENANCE (EXACT SQL EXECUTION):\n",
    "--------------------------------------------------------------------------------\n",
    "1. JIRA SOURCE:\n",
    "   {sql_map['jira']}\n",
    "\n",
    "2. REGISTER SOURCE:\n",
    "   {sql_map['reg']}\n",
    "\n",
    "3. CORE INVENTORY SOURCE:\n",
    "   {sql_map['core']}\n",
    "\n",
    "LOGIC HIERARCHY:\n",
    "--------------------------------------------------------------------------------\n",
    "1. B1 (Reg->Jira): Match Register Name to Jira Summary. Priority: Direct NIAP ID > Fuzzy Match.\n",
    "2. B2 (Core->Reg): Match Cleaned Core Name to Register Name. If found, inherit Jira Ticket.\n",
    "3. B3 (Core->Jira): Match Cleaned Core Name to Jira Summary. Only runs if B2 fails.\n",
    "\"\"\"\n",
    "    with open(TXT_METADATA, 'w') as f: f.write(content)\n",
    "\n",
    "def generate_deep_dive_report(metrics_a, metrics_b):\n",
    "    # 1. Jira Status Table\n",
    "    jira_table = \"| Status | Total Tickets | Linked to Reg | Coverage % |\\n|---|---|---|---|\\n\"\n",
    "    for status, data in metrics_a['a1_jira_governance']['linkage_analysis'].items():\n",
    "        jira_table += f\"| {status} | {data['total_tickets']} | {data['tickets_with_link']} | {data['coverage_pct']}% |\\n\"\n",
    "\n",
    "    # 2. Register Health Table\n",
    "    reg_table = \"| Product Status | Total Records | Populated NIAP IDs |\\n|---|---|---|\\n\"\n",
    "    for status, data in metrics_a['a2_register_health']['id_health_by_status'].items():\n",
    "        reg_table += f\"| {status} | {data['total_records']} | {data['populated_ids']} |\\n\"\n",
    "\n",
    "    # 3. Core Inventory Breakdown\n",
    "    core_table = \"| Product Type | Active Configurations |\\n|---|---|\\n\"\n",
    "    sorted_core = dict(sorted(metrics_a['a3_core_summary']['breakdown_by_type'].items(), key=lambda item: item[1], reverse=True))\n",
    "    for p_type, count in sorted_core.items():\n",
    "        core_table += f\"| {p_type} | {count} |\\n\"\n",
    "\n",
    "    # 4. Gap Analysis Table\n",
    "    gap_table = \"| Product Type | Risk Volume (Unmapped) |\\n|---|---|\\n\"\n",
    "    gap_data = metrics_b['master_summary']['gap_risk_volume_by_type']\n",
    "    for p_type, vol in gap_data.items():\n",
    "        gap_table += f\"| {p_type} | {vol} |\\n\"\n",
    "\n",
    "    md = f\"\"\"# NIAP Audit: Deep Dive Analysis Report\n",
    "**Timestamp:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "## 1. Executive Summary\n",
    "| Metric | Value |\n",
    "|---|---|\n",
    "| **Total Product Families** | {metrics_b['master_summary']['total_families']} |\n",
    "| **Compliant Families** | {metrics_b['master_summary']['compliant']} |\n",
    "| **Gap Families** | {metrics_b['master_summary']['gaps']} |\n",
    "| **Total Risk Volume** | {metrics_a['a3_core_summary']['total_configs']} |\n",
    "\n",
    "## 2. Phase A: Source Data Health\n",
    "\n",
    "### 2.1 Jira Governance Linkage (By Status)\n",
    "{jira_table}\n",
    "\n",
    "### 2.2 Product Registry Health (By Status)\n",
    "{reg_table}\n",
    "\n",
    "### 2.3 Core Inventory Composition (Risk Profile)\n",
    "{core_table}\n",
    "\n",
    "## 3. Phase B: Mapping & Gap Analysis\n",
    "\n",
    "### 3.1 Unmapped Risk Volumes (Gaps)\n",
    "{gap_table}\n",
    "\"\"\"\n",
    "    with open(REPORT_MD, 'w') as f: f.write(md)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROBUST UPLOAD WRAPPER\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_to_drive(file_path, mime_type):\n",
    "    max_retries = 3\n",
    "    print(f\" -> Uploading {file_path}...\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            upload_file_to_gdrive(file=file_path, name=file_path, mime_type=mime_type, folder_id=TARGET_FOLDER_ID)\n",
    "            return # Success\n",
    "        except NameError:\n",
    "            print(f\"    [WARN] Upload skipped (Local Mode).\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"    [WARN] Attempt {attempt+1}/{max_retries} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(5) # Wait 5 seconds before retrying\n",
    "            else:\n",
    "                print(f\"    [ERROR] Failed to upload {file_path} after {max_retries} attempts.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_master_audit():\n",
    "    try:\n",
    "        print(f\"[{datetime.now()}] Phase A: Extraction...\")\n",
    "        \n",
    "        queries = {\n",
    "            \"jira\": \"SELECT issue_id, summary, issue_status, product_register_link FROM global_entity_operations.jira_niap_initiatives\",\n",
    "            \"reg\": \"SELECT name, niap, product_status FROM global_entity_operations.niap_product_register\",\n",
    "            \"core\": \"SELECT product_type, name, COUNT(*) as configuration_count FROM core.products WHERE (decommission_date IS NULL OR decommission_date > CURRENT_DATE) GROUP BY 1, 2\"\n",
    "        }\n",
    "\n",
    "        with zeus() as cur:\n",
    "            df_jira = execute_sql(cur, queries[\"jira\"], None)\n",
    "            df_reg  = execute_sql(cur, queries[\"reg\"], None)\n",
    "            df_core = execute_sql(cur, queries[\"core\"], None)\n",
    "\n",
    "        # --- METRICS A ---\n",
    "        df_jira['has_reg_link'] = df_jira['product_register_link'].notna() & (df_jira['product_register_link'].astype(str) != 'None')\n",
    "        jira_stats = df_jira.groupby('issue_status').agg(total_tickets=('issue_id', 'count'), tickets_with_link=('has_reg_link', 'sum'))\n",
    "        jira_stats['coverage_pct'] = (jira_stats['tickets_with_link'] / jira_stats['total_tickets'] * 100).round(2)\n",
    "\n",
    "        df_reg['niap_populated'] = df_reg['niap'].notna() & (~df_reg['niap'].astype(str).isin(['None', 'nan', '']))\n",
    "        df_reg['product_status'] = df_reg['product_status'].fillna('Unknown')\n",
    "        reg_health = df_reg.groupby('product_status').agg(total_records=('name', 'count'), populated_ids=('niap_populated', 'sum'))\n",
    "\n",
    "        metrics_a = {\n",
    "            \"a1_jira_governance\": {\"total_initiatives\": len(df_jira), \"linkage_analysis\": jira_stats.to_dict('index')},\n",
    "            \"a2_register_health\": {\"total\": len(df_reg), \"id_health_by_status\": reg_health.to_dict('index')},\n",
    "            \"a3_core_summary\": {\n",
    "                \"total_configs\": int(df_core['configuration_count'].sum()),\n",
    "                \"breakdown_by_type\": df_core.groupby('product_type')['configuration_count'].sum().to_dict()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # --- PHASE B1 (Reg -> Jira) ---\n",
    "        print(f\"[{datetime.now()}] Phase B1: Register Mapping...\")\n",
    "        j_matcher = SmartMatcher(df_jira['summary'].tolist())\n",
    "        j_id_map = dict(zip(df_jira['summary'], df_jira['issue_id']))\n",
    "        j_status_map = dict(zip(df_jira['issue_id'], df_jira['issue_status']))\n",
    "        \n",
    "        b1_results = {}\n",
    "        b1_matches = j_matcher.match(df_reg['name'].tolist())\n",
    "        \n",
    "        for i, row in df_reg.iterrows():\n",
    "            name = str(row['name'])\n",
    "            key = str(row['niap']) if pd.notna(row['niap']) else \"nan\"\n",
    "            match = b1_matches[i]\n",
    "            if key in j_status_map: res = {\"ticket\": key, \"method\": \"Direct Link\", \"conf\": 100}\n",
    "            elif match['best_match']: res = {\"ticket\": j_id_map.get(match['best_match']), \"method\": \"Fuzzy (WRatio)\", \"conf\": match['score']}\n",
    "            else: res = {\"ticket\": \"-\", \"method\": \"GAP\", \"conf\": 0}\n",
    "            b1_results[name.lower()] = res\n",
    "\n",
    "        # --- PHASE B2/B3 (Core -> Reg -> Jira) ---\n",
    "        print(f\"[{datetime.now()}] Phase B2/B3: Hierarchical Mapping...\")\n",
    "        core_families = df_core.groupby(['product_type', 'name'])['configuration_count'].sum().reset_index()\n",
    "        core_families['clean_name'] = core_families['name'].apply(clean_technical_name)\n",
    "        \n",
    "        r_matcher = SmartMatcher(df_reg['name'].tolist())\n",
    "        b2_matches = r_matcher.match(core_families['clean_name'].tolist(), threshold=85)\n",
    "        b3_matches = j_matcher.match(core_families['clean_name'].tolist(), threshold=85)\n",
    "\n",
    "        master_results = []\n",
    "        for i, c_row in core_families.iterrows():\n",
    "            ticket, path, best_reg = \"-\", \"GAP\", \"-\"\n",
    "            \n",
    "            if b2_matches[i]['best_match']:\n",
    "                best_reg = b2_matches[i]['best_match']\n",
    "                link = b1_results.get(best_reg.lower())\n",
    "                if link and link['ticket'] != \"-\": ticket, path = link['ticket'], f\"Inherited via Register ({link['method']})\"\n",
    "            \n",
    "            if ticket == \"-\" and b3_matches[i]['best_match']:\n",
    "                 found_id = j_id_map.get(b3_matches[i]['best_match'])\n",
    "                 if found_id: ticket, path = found_id, \"Direct Match to Jira\"\n",
    "\n",
    "            master_results.append({\n",
    "                \"product_family\": c_row['name'], \"product_type\": c_row['product_type'], \"risk_volume\": int(c_row['configuration_count']), \n",
    "                \"mapped_reg_entry\": best_reg, \"evidence_ticket\": ticket, \"status\": j_status_map.get(ticket, \"N/A\"), \"traceability_path\": path\n",
    "            })\n",
    "\n",
    "        # --- FINAL OUTPUTS ---\n",
    "        print(f\"[{datetime.now()}] Generating Deep Dive Reports...\")\n",
    "        df_master = pd.DataFrame(master_results)\n",
    "        df_master.to_csv(CSV_MASTER, index=False)\n",
    "        \n",
    "        gap_data = df_master[df_master['evidence_ticket'] == \"-\"]\n",
    "        metrics_b = {\n",
    "            \"b1_registry_health\": b1_results,\n",
    "            \"master_summary\": {\n",
    "                \"total_families\": len(df_master), \"compliant\": len(df_master) - len(gap_data), \"gaps\": len(gap_data),\n",
    "                \"gap_risk_volume_by_type\": gap_data.groupby('product_type')['risk_volume'].sum().sort_values(ascending=False).to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(JSON_METRICS, 'w') as f: json.dump(metrics_a, f, indent=4, cls=NpEncoder)\n",
    "        with open(JSON_ANALYSIS, 'w') as f: json.dump(metrics_b, f, indent=4, cls=NpEncoder)\n",
    "        generate_audit_manifest(str(datetime.now()), queries)\n",
    "        generate_deep_dive_report(metrics_a, metrics_b)\n",
    "        \n",
    "        df_jira.to_csv(CSV_JIRA, index=False) \n",
    "        df_reg.to_csv(CSV_REG, index=False)\n",
    "        df_core.to_csv(CSV_CORE, index=False)\n",
    "\n",
    "        print(f\"[{datetime.now()}] Uploading 8 Artifacts...\")\n",
    "        artifacts = [\n",
    "            (CSV_MASTER, \"text/csv\"), (JSON_METRICS, \"application/json\"), \n",
    "            (JSON_ANALYSIS, \"application/json\"), (TXT_METADATA, \"text/plain\"), \n",
    "            (REPORT_MD, \"text/markdown\"), (CSV_JIRA, \"text/csv\"), \n",
    "            (CSV_REG, \"text/csv\"), (CSV_CORE, \"text/csv\")\n",
    "        ]\n",
    "        for f_path, f_mime in artifacts:\n",
    "            upload_to_drive(f_path, f_mime)\n",
    "\n",
    "        print(f\"\\n✅ DEEP DIVE AUDIT COMPLETE. Report: {REPORT_MD}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ EXECUTION FAILED: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_master_audit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
